{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reimplementation of ML Model by Hou et al.\n",
    "\n",
    "This notebook implements a model presented by Hou et al. in [Towards Automatic Detection of Misinformation in Online Medical Videos](https://arxiv.org/pdf/1909.01543.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "We train and evaluate the following models:\n",
    "\n",
    "1. Original version published by Hou et al. using SVM classifier: `LinearSVC` model from `sklearn` with `C=1` and L2 normalizer applied to features. We compare binary, binary with neutral and ternary variants of the model.\n",
    "2. Modified version of the model using XGBosst classifier (binary and binary with neutral variants): `XGBClassifier` from `xgboost` with the following hyperparameters: `'booster': 'gbtree', 'random_state': 0, 'objective': 'binary:logistic', 'learning_rate': 0.1, 'n_estimators': 500, 'max_depth': 10, 'min_child_weight': 1, 'gamma': 0, 'subsample': 0.8, 'colsample_bytree': 0.8`. L2 normalizer is applied to the features as well.\n",
    "3. Modified version using XGBosst classifier (ternary variant): `XGBClassifier` from `xgboost` with the following hyperparameters: `'booster': 'gbtree', 'random_state': 0, 'objective': 'multi:softprob', 'learning_rate': 0.1, 'n_estimators': 500, 'max_depth': 10, 'min_child_weight': 1, 'gamma': 0, 'subsample': 0.8, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss'`. L2 normalizer is applied to the features as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "### Stats\n",
    "\n",
    "* view count per day\n",
    "* comment count\n",
    "* like count\n",
    "* dislike count\n",
    "* duration in seconds\n",
    "\n",
    "Missing:\n",
    "* categories were not used since they were missing in our data\n",
    "### Linguistic\n",
    "\n",
    "* ngrams – `TfidfVectorizer` using English stopwords and 1 and 2-grams limited to 1000 features\n",
    "* readability – all measures from the `readability` library\n",
    "* liwc – percentage of token counts by categories in the LIWC lexicon\n",
    "\n",
    "### Acoustic – not implemented\n",
    "\n",
    "Although the paper applied also acoustic features, we did not evaluate these since we did not collect sound from YouTube videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import isodate\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "import liwc\n",
    "from nltk.tokenize import word_tokenize\n",
    "import readability\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Load a dataset of videos into `videos` pandas DataFrame. The provided training data consist of our seed and encountered videos that we manually annotated and for which we were able to obtain metadata via YouTube API. We publish only `youtube_id` and `annotation` columns. For the rest, please use the official YouTube API (please note that some videos might no longer be available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = pd.read_csv('../Data/normalized_data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2622 entries, 0 to 2621\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   youtube_id       2622 non-null   object \n",
      " 1   published_at     0 non-null      float64\n",
      " 2   updated_at       0 non-null      float64\n",
      " 3   view_count       0 non-null      float64\n",
      " 4   like_count       0 non-null      float64\n",
      " 5   dislike_count    0 non-null      float64\n",
      " 6   favourite_count  0 non-null      float64\n",
      " 7   comment_count    0 non-null      float64\n",
      " 8   duration         0 non-null      float64\n",
      " 9   transcript       0 non-null      float64\n",
      " 10  annotation       2622 non-null   object \n",
      "dtypes: float64(9), object(2)\n",
      "memory usage: 225.5+ KB\n"
     ]
    }
   ],
   "source": [
    "videos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>youtube_id</th>\n",
       "      <th>published_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>favourite_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>duration</th>\n",
       "      <th>transcript</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1w0_kazbb_U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>promoting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R9oqi6HteJg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>debunking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67ZKmVWB3tY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>promoting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zw0nYNMUIfA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>promoting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e20vaAtncsM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>debunking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    youtube_id  published_at  updated_at  view_count  like_count  \\\n",
       "0  1w0_kazbb_U           NaN         NaN         NaN         NaN   \n",
       "1  R9oqi6HteJg           NaN         NaN         NaN         NaN   \n",
       "2  67ZKmVWB3tY           NaN         NaN         NaN         NaN   \n",
       "3  zw0nYNMUIfA           NaN         NaN         NaN         NaN   \n",
       "4  e20vaAtncsM           NaN         NaN         NaN         NaN   \n",
       "\n",
       "   dislike_count  favourite_count  comment_count  duration  transcript  \\\n",
       "0            NaN              NaN            NaN       NaN         NaN   \n",
       "1            NaN              NaN            NaN       NaN         NaN   \n",
       "2            NaN              NaN            NaN       NaN         NaN   \n",
       "3            NaN              NaN            NaN       NaN         NaN   \n",
       "4            NaN              NaN            NaN       NaN         NaN   \n",
       "\n",
       "  annotation  \n",
       "0  promoting  \n",
       "1  debunking  \n",
       "2  promoting  \n",
       "3  promoting  \n",
       "4  debunking  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral      1459\n",
       "debunking     758\n",
       "promoting     405\n",
       "Name: annotation, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos['annotation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_tags(text):\n",
    "    \"\"\"\n",
    "    Remove vtt markup tags\n",
    "    \"\"\"\n",
    "    tags = [\n",
    "        r'</c>',\n",
    "        r'<c(\\.color\\w+)?>',\n",
    "        r'<\\d{2}:\\d{2}:\\d{2}\\.\\d{3}>',\n",
    "\n",
    "    ]\n",
    "\n",
    "    for pat in tags:\n",
    "        text = re.sub(pat, '', text)\n",
    "\n",
    "    # extract timestamp, only kep HH:MM\n",
    "    text = re.sub(\n",
    "        r'(\\d{2}:\\d{2}):\\d{2}\\.\\d{3} --> .* align:start position:0%',\n",
    "        r'\\g<1>',\n",
    "        text\n",
    "    )\n",
    "\n",
    "    text = re.sub(r'^\\s+$', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def remove_header(lines):\n",
    "    \"\"\"\n",
    "    Remove vtt file header\n",
    "    \"\"\"\n",
    "    pos = -1\n",
    "    for mark in ('##', 'Language: en',):\n",
    "        if mark in lines:\n",
    "            pos = lines.index(mark)\n",
    "    lines = lines[pos+1:]\n",
    "    return lines\n",
    "\n",
    "\n",
    "def merge_duplicates(lines):\n",
    "    \"\"\"\n",
    "    Remove duplicated subtitles. Duplacates are always adjacent.\n",
    "    \"\"\"\n",
    "    last_timestamp = ''\n",
    "    last_cap = ''\n",
    "    for line in lines:\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        if re.match('^\\d{2}:\\d{2}$', line):\n",
    "            if line != last_timestamp:\n",
    "                last_timestamp = line\n",
    "        else:\n",
    "            if line != last_cap:\n",
    "                yield line\n",
    "                last_cap = line\n",
    "\n",
    "\n",
    "def merge_short_lines(lines):\n",
    "    buffer = ''\n",
    "    for line in lines:\n",
    "        if line == \"\" or re.match('^\\d{2}:\\d{2}$', line):\n",
    "            yield '\\n' + line\n",
    "            continue\n",
    "\n",
    "        if len(line+buffer) < 80:\n",
    "            buffer += ' ' + line\n",
    "        else:\n",
    "            yield buffer.strip()\n",
    "            buffer = line\n",
    "    yield buffer\n",
    "\n",
    "\n",
    "def parse_transcript(text):\n",
    "    text = remove_tags(text)\n",
    "    lines = text.splitlines()\n",
    "    lines = remove_header(lines)\n",
    "    lines = merge_duplicates(lines)\n",
    "    lines = list(lines)\n",
    "    lines = merge_short_lines(lines)\n",
    "    lines = list(lines)\n",
    "    result = ' '.join(lines)\n",
    "    return re.sub('\\d{2}:\\d{2}:\\d{2}\\.\\d{3} --> \\d{2}:\\d{2}:\\d{2}\\.\\d{3} ', '', result)\n",
    "\n",
    "videos['transcript'] = videos['transcript'].fillna('')\n",
    "videos['clean_transcript'] = videos['transcript'].apply(lambda transcript: parse_transcript(transcript))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_cols = ['view_count', 'like_count', 'dislike_count', 'favourite_count', 'comment_count']\n",
    "videos[count_cols] = videos[count_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate counts of word classes in transcript using the LIWC lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse, category_names = liwc.load_token_parser('LIWC2007_English100131.dic')\n",
    "lexicon, _ = liwc.dic.read_dic('LIWC2007_English100131.dic')\n",
    "\n",
    "liwc_category_counts = Counter(\n",
    "    value\n",
    "    for key, values in lexicon.items()\n",
    "    for value in values\n",
    ")\n",
    "\n",
    "def compute_liwc_transcript_counts(videos):\n",
    "    liwc_transcript_counts = videos['clean_transcript'].apply(\n",
    "        lambda transcript: pd.DataFrame({\n",
    "            (category, token)\n",
    "            for token in word_tokenize(transcript)\n",
    "            for category in parse(token.lower())\n",
    "        }, columns=['category', 'token']).groupby('category').size()\n",
    "    ).fillna(0)\n",
    "\n",
    "    for column in liwc_transcript_counts.columns:\n",
    "        liwc_transcript_counts[column] = liwc_transcript_counts[column] / liwc_category_counts[column]\n",
    "    \n",
    "    return liwc_transcript_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate readability of transcript using the readability package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_readability(videos):\n",
    "    readability_scores = videos['clean_transcript'].apply(\n",
    "        lambda transcript: pd.Series({\n",
    "            f'{k1}-{k2}': v\n",
    "            for k1, vs in readability.getmeasures(transcript, lang='en').items()\n",
    "            for k2, v in vs.items()\n",
    "        } if len(transcript) > 0 else {}, dtype='float64')\n",
    "    ).fillna(0)\n",
    "    readability_scores.index = videos.index\n",
    "\n",
    "    return readability_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the statistical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(videos):\n",
    "    videos['num_tracked_days'] = (\n",
    "        pd.to_datetime(videos['updated_at'], utc=True) - pd.to_datetime(videos['published_at'], utc=True)\n",
    "    ).dt.days\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'view_count': videos['view_count'] / videos['num_tracked_days'],\n",
    "        'comment_count': videos['comment_count'],\n",
    "        'like_count': videos['like_count'],\n",
    "        'dislike_count': videos['dislike_count'],\n",
    "        'duration': videos['duration'].apply(isodate.parse_duration).dt.total_seconds(),\n",
    "        'clean_transcript': videos['clean_transcript']\n",
    "    }).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The machine learning pipeline for different combinations of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classifier(clf):\n",
    "    if clf['clf_type'] == 'svm':\n",
    "        return LinearSVC(**clf['params'])\n",
    "    elif clf['clf_type'] == 'xgboost':\n",
    "        return XGBClassifier(**clf['params'])\n",
    "\n",
    "    # default classifier\n",
    "    return LinearSVC(random_state=0, C=1)\n",
    "\n",
    "def clf_pipeline(column_transformer, classifier, sampler=None):\n",
    "    \n",
    "    if sampler:\n",
    "        return make_pipeline(\n",
    "            sampler,\n",
    "            column_transformer,\n",
    "            make_classifier(classifier)\n",
    "        )\n",
    "    else:\n",
    "        return make_pipeline(\n",
    "            column_transformer,\n",
    "            make_classifier(classifier)\n",
    "        )\n",
    "\n",
    "def make_clf_pipelines(X, stats, readability_scores, liwc_transcript_counts, samplers=['no-sampling'], col_transformers=['full'], classifiers=['svm']):\n",
    "    all_samplers = {\n",
    "        'no-sampling':   None,\n",
    "        'oversampling':  RandomOverSampler(sampling_strategy='not majority'),\n",
    "        'undersampling': RandomUnderSampler(sampling_strategy='not minority', replacement=False)\n",
    "    }\n",
    "\n",
    "    all_column_transformers = {\n",
    "        'full': make_column_transformer(\n",
    "                (\n",
    "                    make_pipeline(\n",
    "                        TfidfVectorizer(\n",
    "                            stop_words='english',\n",
    "                            ngram_range=(1, 2),\n",
    "                            max_features=1000\n",
    "                        ),\n",
    "                        Normalizer(norm='l2')\n",
    "                    ),\n",
    "                    'clean_transcript'\n",
    "                ),\n",
    "                (\n",
    "                    Normalizer(norm='l2'),\n",
    "                    list(set(X.columns).difference(['clean_transcript']))\n",
    "                )\n",
    "        ),\n",
    "        'ngrams': make_column_transformer(\n",
    "               (\n",
    "                   make_pipeline(\n",
    "                       TfidfVectorizer(\n",
    "                           stop_words='english',\n",
    "                           ngram_range=(1, 2),\n",
    "                           max_features=1000\n",
    "                       ),\n",
    "                       Normalizer(norm='l2')\n",
    "                   ),\n",
    "                   'clean_transcript'\n",
    "               )\n",
    "        ),\n",
    "        'stats': make_column_transformer(\n",
    "               (\n",
    "                   Normalizer(norm='l2'),\n",
    "                   list(set(stats.columns).difference(['clean_transcript']))\n",
    "               )\n",
    "        ),\n",
    "        'readability': make_column_transformer(\n",
    "               (\n",
    "                   Normalizer(norm='l2'),\n",
    "                   list(readability_scores.columns)\n",
    "               )\n",
    "        ),\n",
    "        'liwc': make_column_transformer(\n",
    "               (\n",
    "                   Normalizer(norm='l2'),\n",
    "                   list(liwc_transcript_counts.columns)\n",
    "               )\n",
    "        )\n",
    "    }\n",
    "\n",
    "    all_classifiers = {\n",
    "        'svm': {\n",
    "           'clf_type': 'svm',\n",
    "           'params': {'random_state': 0, 'C': 1}\n",
    "        },\n",
    "        'xgboost_binary': {\n",
    "           'clf_type': 'xgboost',\n",
    "           'params': {'booster': 'gbtree', 'random_state': 0, 'objective': 'binary:logistic', 'learning_rate': 0.1, \n",
    "           'n_estimators': 500, 'max_depth': 10, 'min_child_weight': 1, 'gamma': 0, 'subsample': 0.8, \n",
    "           'colsample_bytree': 0.8}\n",
    "        },\n",
    "        'xgboost_ternary': {\n",
    "            'clf_type': 'xgboost',\n",
    "            'params': {'booster': 'gbtree', 'random_state': 0, 'objective': 'multi:softprob', 'learning_rate': 0.1, \n",
    "            'n_estimators': 500, 'max_depth': 10, 'min_child_weight': 1, 'gamma': 0, 'subsample': 0.8, \n",
    "            'colsample_bytree': 0.8, 'eval_metric': 'mlogloss'}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    clfs = {}\n",
    "\n",
    "    for sampler_key, sampler in all_samplers.items():\n",
    "        if sampler_key not in samplers:\n",
    "            continue\n",
    "\n",
    "        for col_transformer_key, col_transformer in all_column_transformers.items():\n",
    "            if col_transformer_key not in col_transformers:\n",
    "                continue\n",
    "            \n",
    "            for clf_key, classifier in all_classifiers.items():\n",
    "                if clf_key not in classifiers:\n",
    "                    continue\n",
    "                clfs[f\"{sampler_key}_{col_transformer_key}_{clf_key}\"] = clf_pipeline(col_transformer, classifier, sampler)\n",
    "\n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validate the pipelines and output the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_cross_val_predictions(X, y, clfs):\n",
    "    print('Classification reports')\n",
    "    print('----------------------------')\n",
    "    print()\n",
    "\n",
    "    predicted = {}\n",
    "    for label, clf in clfs.items():\n",
    "        print(label)\n",
    "        predicted[label] = cross_val_predict(clf, X, y, cv=5)\n",
    "        print(classification_report(y, predicted[label]))\n",
    "        print()\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary (without neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_binary = videos.loc[videos['annotation'].isin(['promoting', 'debunking'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(videos_binary.shape[0], 'videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readability_scores_binary = compute_readability(videos_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_transcript_counts_binary = compute_liwc_transcript_counts(videos_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_binary = compute_stats(videos_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_binary = pd.concat([stats_binary, readability_scores_binary, liwc_transcript_counts_binary], axis=1)\n",
    "y_binary = videos_binary['annotation']\n",
    "X_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplers_binary = ['no-sampling']\n",
    "# samplers_binary = ['no-sampling', 'oversampling', 'undersampling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_transformers_binary = ['full']\n",
    "# col_transformers_binary = ['full', 'ngrams', 'stats', 'readability', 'liwc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_binary = ['svm', 'xgboost_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs_binary = make_clf_pipelines(\n",
    "    X_binary, stats_binary, readability_scores_binary, liwc_transcript_counts_binary, \n",
    "    samplers=samplers_binary, col_transformers=col_transformers_binary, classifiers=classifiers_binary)\n",
    "clfs_binary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binary_transformed = list(map(lambda x: 1 if x == 'promoting' else 0, y_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_binary = compute_cross_val_predictions(X_binary, y_binary_transformed, clfs_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary (with neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_binary_neutral = videos.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_binary_neutral.loc[videos_binary_neutral['annotation'] == 'neutral', ['annotation']] = 'debunking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_binary_neutral = videos_binary_neutral.loc[videos_binary_neutral['annotation'].isin(['promoting', 'debunking'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_binary_neutral['annotation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(videos_binary_neutral.shape[0], 'videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readability_scores_binary_neutral = compute_readability(videos_binary_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_transcript_counts_binary_neutral = compute_liwc_transcript_counts(videos_binary_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_binary_neutral = compute_stats(videos_binary_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_binary_neutral = pd.concat([\n",
    "    stats_binary_neutral, readability_scores_binary_neutral, liwc_transcript_counts_binary_neutral\n",
    "], axis=1)\n",
    "y_binary_neutral = videos_binary_neutral['annotation']\n",
    "X_binary_neutral.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplers_binary_neutral = ['no-sampling']\n",
    "# samplers_binary_neutral = ['no-sampling', 'oversampling', 'undersampling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_transformers_binary_neutral = ['full']\n",
    "# col_transformers_binary_neutral = ['full', 'ngrams', 'stats', 'readability', 'liwc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_binary_neutral = ['svm', 'xgboost_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs_binary_neutral = make_clf_pipelines(\n",
    "    X_binary_neutral, stats_binary_neutral, readability_scores_binary_neutral, \n",
    "    liwc_transcript_counts_binary_neutral,\n",
    "    samplers=samplers_binary_neutral, col_transformers=col_transformers_binary_neutral, classifiers=classifiers_binary_neutral\n",
    ")\n",
    "clfs_binary_neutral.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binary_neutral_transformed = list(map(lambda x: 1 if x == 'promoting' else 0, y_binary_neutral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_binary_neutral = compute_cross_val_predictions(X_binary_neutral, y_binary_neutral_transformed, clfs_binary_neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ternary (three classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_ternary = videos.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_ternary = videos_ternary.loc[videos_ternary['annotation'].isin(['promoting', 'debunking', 'neutral'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_ternary['annotation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(videos_ternary.shape[0], 'videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readability_scores_ternary = compute_readability(videos_ternary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_transcript_counts_ternary = compute_liwc_transcript_counts(videos_ternary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_ternary = compute_stats(videos_ternary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ternary = pd.concat([\n",
    "    stats_ternary, readability_scores_ternary, liwc_transcript_counts_ternary\n",
    "], axis=1)\n",
    "y_ternary = videos_ternary['annotation']\n",
    "X_ternary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplers_ternary = ['no-sampling']\n",
    "# samplers_ternary = ['no-sampling', 'oversampling', 'undersampling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_transformers_ternary = ['full']\n",
    "# col_transformers_ternary = ['full', 'ngrams', 'stats', 'readability', 'liwc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_ternary = ['svm', 'xgboost_ternary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs_ternary = make_clf_pipelines(\n",
    "    X_ternary, stats_ternary, readability_scores_ternary,liwc_transcript_counts_ternary,\n",
    "    samplers=samplers_ternary, col_transformers=col_transformers_ternary, classifiers=classifiers_ternary\n",
    ")\n",
    "clfs_ternary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ternary_labels(label):\n",
    "    if label == 'neutral':\n",
    "        return 0\n",
    "    if label == 'debunking':\n",
    "        return 1\n",
    "    if label == 'promoting':\n",
    "        return 2\n",
    "y_ternary_transformed = list(map(map_ternary_labels, y_ternary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ternary = compute_cross_val_predictions(X_ternary, y_ternary_transformed, clfs_ternary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "57e8ab2c44c7cc3097d31685c867aa29b59a70c4aee7409180d9015f3b8ee2df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
