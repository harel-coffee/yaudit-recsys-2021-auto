{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reimplementation of ML Model by Hou et al.\n",
    "\n",
    "Implementation of a SVM model presented in [Towards Automatic Detection of Misinformation in Online Medical Videos](https://arxiv.org/pdf/1909.01543.pdf).\n",
    "\n",
    "The model is a LinearSVC model from sklearn with C=1. L2 normalizer is applied to features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import isodate\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "import liwc\n",
    "from nltk.tokenize import word_tokenize\n",
    "import readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Load a dataset of videos into `videos` pandas DataFrame with the following columns and sample values:\n",
    "\n",
    "```\n",
    "published_at                                         2013-12-23 03:21:21+01:00\n",
    "view_count                                                           1328337.0\n",
    "like_count                                                             30946.0\n",
    "dislike_count                                                            706.0\n",
    "favourite_count                                                              0\n",
    "comment_count                                                           4254.0\n",
    "category_id                                                               None\n",
    "updated_at                                    2021-06-03 10:46:58.939401+02:00\n",
    "clean_transcript             Translator: Delia Bogdan Reviewer: Ilze Garda ...\n",
    "annotation                                                           promoting\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate counts of word classes in transcript using the LIWC lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse, category_names = liwc.load_token_parser('LIWC2007_English100131.dic')\n",
    "lexicon, _ = liwc.dic.read_dic('LIWC2007_English100131.dic')\n",
    "\n",
    "liwc_category_counts = Counter(\n",
    "    value\n",
    "    for key, values in lexicon.items()\n",
    "    for value in values\n",
    ")\n",
    "\n",
    "liwc_transcript_counts = videos['clean_transcript'].apply(\n",
    "    lambda transcript: pd.DataFrame({\n",
    "        (category, token)\n",
    "        for token in word_tokenize(transcript)\n",
    "        for category in parse(token.lower())\n",
    "    }, columns=['category', 'token']).groupby('category').size()\n",
    ").fillna(0)\n",
    "\n",
    "for column in liwc_transcript_counts.columns:\n",
    "    liwc_transcript_counts[column] = liwc_transcript_counts[column] / liwc_category_counts[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate readability of transcript using the readability package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readability_scores = videos['clean_transcript'].apply(\n",
    "    lambda transcript: pd.Series({\n",
    "        f'{k1}-{k2}': v\n",
    "        for k1, vs in readability.getmeasures(transcript, lang='en').items()\n",
    "        for k2, v in vs.items()\n",
    "    } if len(transcript) > 0 else {})\n",
    ").fillna(0)\n",
    "readability_scores.index = videos.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine features into dataframes X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos['num_tracked_days'] = (\n",
    "    pd.to_datetime(videos['updated_at'], utc=True) - pd.to_datetime(videos['published_at'], utc=True)\n",
    ").dt.days\n",
    "\n",
    "stats = pd.DataFrame({\n",
    "    'view_count': videos['view_count'] / videos['num_tracked_days'],\n",
    "    'comment_count': videos['comment_count'],\n",
    "    'like_count': videos['like_count'],\n",
    "    'dislike_count': videos['dislike_count'],\n",
    "    'duration': videos['duration'].apply(isodate.parse_duration).dt.total_seconds(),\n",
    "    # videos['category_id']\n",
    "    'clean_transcript': videos['clean_transcript']\n",
    "}).fillna(0)\n",
    "X = pd.concat([stats, readability_scores, liwc_transcript_counts], axis=1)\n",
    "y = videos['annotation']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The machine learning pipeline for different combinations of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_pipeline(column_transformer):\n",
    "    return make_pipeline(\n",
    "        RandomOverSampler(sampling_strategy='not majority'),\n",
    "        column_transformer,\n",
    "        LinearSVC(\n",
    "            random_state=0,\n",
    "            C=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "clfs = {\n",
    "    'full': clf_pipeline(\n",
    "        make_column_transformer(\n",
    "            (\n",
    "                make_pipeline(\n",
    "                    TfidfVectorizer(\n",
    "                        stop_words='english',\n",
    "                        ngram_range=(1, 2),\n",
    "                        max_features=1000\n",
    "                    ),\n",
    "                    Normalizer(norm='l2')\n",
    "                ),\n",
    "                'clean_transcript'\n",
    "            ),\n",
    "            (\n",
    "                Normalizer(norm='l2'),\n",
    "                list(set(X.columns).difference(['clean_transcript']))\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    'ngrams': clf_pipeline(\n",
    "        make_column_transformer(\n",
    "            (\n",
    "                make_pipeline(\n",
    "                    TfidfVectorizer(\n",
    "                        stop_words='english',\n",
    "                        ngram_range=(1, 2),\n",
    "                        max_features=1000\n",
    "                    ),\n",
    "                    Normalizer(norm='l2')\n",
    "                ),\n",
    "                'clean_transcript'\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    'stats': clf_pipeline(\n",
    "        make_column_transformer(\n",
    "            (\n",
    "                Normalizer(norm='l2'),\n",
    "                list(set(stats.columns).difference(['clean_transcript']))\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    'readability': clf_pipeline(\n",
    "        make_column_transformer(\n",
    "            (\n",
    "                Normalizer(norm='l2'),\n",
    "                list(readability_scores.columns)\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    'liwc': clf_pipeline(\n",
    "        make_column_transformer(\n",
    "            (\n",
    "                Normalizer(norm='l2'),\n",
    "                list(liwc_transcript_counts.columns)\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validate the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "sampling = ~y.isna()\n",
    "predicted = {}\n",
    "for label, clf in clfs.items():\n",
    "    print(label)\n",
    "    predicted[label] = cross_val_predict(clf, X.loc[sampling], y.loc[sampling], cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for label, y_pred in predicted.items():\n",
    "    print(label)\n",
    "    print(classification_report(y.loc[sampling], y_pred))\n",
    "    print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
